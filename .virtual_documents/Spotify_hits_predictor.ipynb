








import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_score, recall_score, confusion_matrix

import warnings
warnings.filterwarnings("ignore")





dfs = [pd.read_csv(f'datasets/dataset-of-{decade}0s.csv') for decade in ['0', '1','6','7','8','9']]



for i, decade in enumerate([1960,1970,1980,1990,2000, 2010]):
    dfs[i]['decade'] = pd.Series(decade, index=dfs[i].index)

df = pd.concat(dfs, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)


df.head()


df.info()


# Checking null values
df.isna().sum()





# Top 10 artists with most hit songs
top_artists = df['artist'].value_counts().head(20)

plt.figure(figsize=(10, 5))
sns.barplot(x=top_artists.index, y=top_artists.values, palette="magma")
plt.xticks(rotation=90)
plt.title("Top 10 Artists with Most Hit Songs")
plt.xlabel("Artist")
plt.ylabel("Number of Hit Songs")
plt.show()






# Correlation between audio features and song popularity
audio_features = ['danceability', 'energy', 'loudness', 'tempo', 'valence']
correlation = df[audio_features + ['target']].corr()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Between Audio Features and Song Popularity")
plt.show()






#getting number of hits vs non hits

y1 = df[df['target']==1]
y2 = df[df['target']==0]
print(f"The number of hits are: {y1.shape[0]}")
print(f"The number of non hits are: {y2.shape[0]}")



# Plot Grouped Bar Graph
plt.figure(figsize=(6,4))
sns.countplot(x='target', data=df, palette=['blue', 'orange'])
plt.xticks([0, 1], ['Non-Hits', 'Hits'])
plt.xlabel("Song Category")
plt.ylabel("Count")
plt.title("Number of Hits vs Non-Hits")
plt.show()





#selecting features

features = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',
            'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',
            'duration_ms', 'time_signature', 'chorus_hit', 'sections']
target = 'target'


#data preprocessing

X = df[features]
y = df[target]


#standardise numerical features

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


# Apply PCA

pca = PCA(n_components=0.98)  # Use all 4 features first
X_pca = pca.fit_transform(X_scaled)


# Explained variance plot
num_components = len(pca.explained_variance_ratio_)
plt.figure(figsize=(8, 5))
plt.plot(range(1,num_components+1 ), np.cumsum(pca.explained_variance_ratio_), marker="o", linestyle="--")
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("PCA Explained Variance")
plt.show()


# Select optimal number of components

pca_optimal = PCA(n_components=7)
X_pca_opt = pca_optimal.fit_transform(X_scaled)


# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_pca_opt, y, test_size=0.2, random_state=42)


# Model Training and Evaluation
models = {
    "SVM": SVC(),
    "Random Forest": RandomForestClassifier(),
    "Logistic Regression": LogisticRegression(),
    "KNN": KNeighborsClassifier()
}


for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    print(f"{name} - Accuracy: {acc:.4f}, F1-score: {f1:.4f}")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix Heatmap
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='magma', xticklabels=['Non-Hit', 'Hit'], yticklabels=['Non-Hit', 'Hit'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {name}')
    plt.show()



from sklearn.model_selection import RandomizedSearchCV
import scipy.stats as ss


svm_params = {
    "C": ss.uniform(0.01, 10),  # Continuous range for C
    "kernel": ["linear", "rbf", "poly"],
    "gamma": ["scale", "auto"]
}

svm_random = RandomizedSearchCV(SVC(), svm_params, cv=5, scoring="accuracy", n_iter=10, random_state=42)
svm_random.fit(X_train, y_train)

best_svm = svm_random.best_estimator_
print(f"SVM Best Params: {svm_random.best_params_}")
print(f"SVM Best Accuracy: {svm_random.best_score_}")



rf_params = {
    "n_estimators": ss.randint(50, 300),
    "max_depth": ss.randint(5, 30),
    "min_samples_split": ss.randint(2, 10),
}

rf_random = RandomizedSearchCV(RandomForestClassifier(), rf_params, cv=5, n_iter=10, scoring="accuracy", random_state=42)
rf_random.fit(X_train, y_train)

best_rf = rf_random.best_estimator_
print(f"Random Forest Best Params: {rf_random.best_params_}")
print(f"Random Forest Best Accuracy: {rf_random.best_score_}")



knn_params = {
    "n_neighbors": ss.randint(3, 15),
    "metric": ["euclidean", "manhattan", "minkowski"],
    "weights": ["uniform", "distance"]
}

knn_random = RandomizedSearchCV(KNeighborsClassifier(), knn_params, cv=5, n_iter=10, scoring="accuracy", random_state=42)
knn_random.fit(X_train, y_train)

best_knn = knn_random.best_estimator_
print(f"KNN Best Params: {knn_random.best_params_}")
print(f"KNN Best Accuracy: {knn_random.best_score_}")



lr_params = {
    "C": ss.uniform(0.01, 10),  # Continuous range for C
    "penalty": ["l1", "l2"],
    "solver": ["liblinear", "saga"]
}

lr_random = RandomizedSearchCV(LogisticRegression(), lr_params, cv=5, n_iter=10, scoring="accuracy", random_state=42)
lr_random.fit(X_train, y_train)

best_lr = lr_random.best_estimator_
print(f"Logistic Regression Best Params: {lr_random.best_params_}")
print(f"Logistic Regression Best Accuracy: {lr_random.best_score_}")



# Retrain SVM with best params
best_svm = SVC(**svm_random.best_params_)
best_svm.fit(X_train, y_train)

# Retrain Random Forest
best_rf = RandomForestClassifier(**rf_random.best_params_)
best_rf.fit(X_train, y_train)

# Retrain Logistic Regression
best_lr = LogisticRegression(**lr_random.best_params_)
best_lr.fit(X_train, y_train)

# Retrain KNN
best_knn = KNeighborsClassifier(**knn_random.best_params_)
best_knn.fit(X_train, y_train)



print("Final Accuracy After Retraining:")
print(f"SVM: {best_svm.score(X_test, y_test):.4f}")
print(f"Random Forest: {best_rf.score(X_test, y_test):.4f}")
print(f"Logistic Regression: {best_lr.score(X_test, y_test):.4f}")
print(f"KNN: {best_knn.score(X_test, y_test):.4f}")




final_accuracies = {
    'SVM': best_svm.score(X_test, y_test),
    'Random Forest': best_rf.score(X_test, y_test),
    'Logistic Regression': best_lr.score(X_test, y_test),
    'KNN': best_knn.score(X_test, y_test)
}

models = list(final_accuracies.keys())
accuracies = list(final_accuracies.values())

plt.figure(figsize=(10, 6))
plt.bar(models, accuracies, color=['skyblue', 'salmon', 'lightgreen', 'lightcoral'])
plt.xlabel("Models")
plt.ylabel("Accuracy")
plt.title("Final Accuracy of Different Models")
plt.ylim(0.5,1) # Set y-axis limits
plt.show()



summary = pd.DataFrame(final_accuracies, index=['Accuracy']).T
summary




import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_score, recall_score, confusion_matrix

import warnings
warnings.filterwarnings("ignore")


dfs = [pd.read_csv(f'D:\\Coding\\spotify_hits_predictor\\datasets\\dataset-of-{decade}0s.csv') for decade in ['0', '1','6','7','8','9']]


for i, decade in enumerate([1960,1970,1980,1990,2000, 2010]):
    dfs[i]['decade'] = pd.Series(decade, index=dfs[i].index)

df = pd.concat(dfs, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)


df.head()


df.info()


df.isnull().sum() #checking for null values in any if the data that is there in this


y1 = df[df['target']==1]
y2 = df[df['target']==0]
print(f"The number of hits are: {y1.shape[0]}")
print(f"The number of non hits are: {y2.shape[0]}")


# Plot Grouped Bar Graph
plt.figure(figsize=(6,4))
sns.countplot(x='target', data=df, palette=['blue', 'orange'])
plt.xticks([0, 1], ['Non-Hits', 'Hits'])
plt.xlabel("Song Category")
plt.ylabel("Count")
plt.title("Number of Hits vs Non-Hits")
plt.show()


df = df.drop(axis=1, labels='time_signature')


from sklearn.preprocessing import StandardScaler

scale = StandardScaler()
df_X=df.iloc[:,3:17]
df_scaled=scale.fit_transform(df_X)
df_scaled=pd.DataFrame(df_scaled)
df_scaled.columns = df_X.columns
df_scaled


df_melt=pd.melt(df_scaled)
df_melt
sns.set(rc={'figure.figsize':(25,12)})
sns.boxplot(x='variable', y='value', data=df_melt)
plt.xlabel(None)
plt.title('Box plot and Outliers in it')


df_melt=pd.melt(df_scaled)
df_melt
sns.set(rc={'figure.figsize':(25,12)})
sns.boxplot(x='variable', y='value', data=df_melt,showfliers=False)
plt.xlabel(None)
plt.title('Box plot wihtout Outliers in it')


df_final = df_scaled
for col in df_final.columns:
    q1= df_final[col].quantile(0.25)
    q3=df_final[col].quantile(0.75)
    r = q3-q1
    filter=(df_final[col]>=q1-1.5*r) & (df_final[col]<=q3+1.5*r)
    df_final[col]=df_final[col].loc[filter]
    df_final.isna().sum()
   


df_final.isna().sum()


df_final = df_final.drop(axis=1, labels=['instrumentalness']) #highest outlier


df_final = df_final.drop(axis=1, labels=['speechiness']) #second highest


df_f2= df_final.dropna()
df_f2


df_melt=pd.melt(df_f2) #predicotrs without outliers
df_melt
sns.set(rc={'figure.figsize':(30,15)})
sns.boxplot(x='variable',y='value',data=df_melt,showfliers = False)
plt.xlabel(None)
plt.title('Without outliers')


df_merge = df.iloc[df_f2.index]
df_merge = df_merge.drop(axis=1,labels=['speechiness','instrumentalness'])


sns.countplot(x='target', data=df_merge)
sns.set() #no of observations


df_merge


df_merge.groupby('target')[['danceability', 'energy', 'key', 'loudness',
       'mode', 'acousticness', 'liveness', 'valence', 'tempo', 'duration_ms',
       'chorus_hit', 'sections']].count()


df_merge.iloc[:,3:15]


scale = StandardScaler()
scale_X = scale.fit_transform(df_merge.iloc[:,3:15])
df_merge.iloc[:,3:15] = scale_X


data = pd.DataFrame(df_merge)


data.shape[0]


features = ['danceability', 'energy','key','loudness','mode','acousticness','liveness','valence','tempo','duration_ms','chorus_hit','sections']
target = 'target'
X = data[features]
y = data[target]


# Apply PCA

pca = PCA(n_components=0.98)  # Use all 4 features first
X_pca = pca.fit_transform(X)


# Explained variance plot
num_components = len(pca.explained_variance_ratio_)
plt.figure(figsize=(8, 5))
plt.plot(range(1,num_components+1 ), np.cumsum(pca.explained_variance_ratio_), marker="o", linestyle="--")
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("PCA Explained Variance")
plt.show()


# Select optimal number of components 

pca_optimal = PCA(n_components=7)  
X_pca_opt = pca_optimal.fit_transform(X)
X_pca_opt.shape


# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Model Training and Evaluation
models = {
    "SVM": SVC(),
    "Random Forest": RandomForestClassifier(),
    "Logistic Regression": LogisticRegression(),
    "KNN": KNeighborsClassifier()
}


for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    print(f"{name} - Accuracy: {acc:.4f}, F1-score: {f1:.4f}")
    print(classification_report(y_test, y_pred))
    
    # Confusion Matrix Heatmap
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='magma', xticklabels=['Non-Hit', 'Hit'], yticklabels=['Non-Hit', 'Hit'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {name}')
    plt.show()



from sklearn.model_selection import RandomizedSearchCV
import scipy.stats as ss  # For sampling from distributions

svm_params = {
    "C": ss.uniform(0.01, 10),  # Continuous range for C
    "kernel": ["linear", "rbf", "poly"],
    "gamma": ["scale", "auto"]
}

svm_random = RandomizedSearchCV(SVC(), svm_params, cv=5, scoring="accuracy", n_iter=10, random_state=42)
svm_random.fit(X_train, y_train)

best_svm = svm_random.best_estimator_
print(f"SVM Best Params: {svm_random.best_params_}")




